{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c539b31c-c247-4327-ac08-5aee9659e4fa",
   "metadata": {},
   "source": [
    "# LlamaStack 0.3.0 API Testing\n",
    "\n",
    "This notebook demonstrates how to connect to a LlamaStack server and perform basic operations using the **0.3.0 API**.\n",
    "\n",
    "## Key API Changes in 0.3.0\n",
    "\n",
    "| Old (0.2.x) | New (0.3.0) |\n",
    "|-------------|-------------|\n",
    "| `client.inference.chat_completion()` | `client.chat.completions.create()` |\n",
    "| `client.vector_dbs.list()` | `client.vector_stores.list()` |\n",
    "| `vdb.identifier` | `vs.id` |\n",
    "| `vdb.vector_db_name` | `vs.name` |\n",
    "| `rag_tool.query(vector_db_ids=[...])` | `vector_stores.search(vector_store_id=...)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0c611-86b3-497a-9ea0-c6925059c319",
   "metadata": {},
   "source": [
    "Let's start by querying the Llamastack server and check if it is healthy and ready to accept requests. First we need to install the llama-stack client using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c377b52-6d0a-4361-b7be-8f0be0e48a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack-client==0.3.0 in ./venv/lib/python3.12/site-packages (0.3.0)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (14.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (4.12.1)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (8.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (1.9.0)\n",
      "Requirement already satisfied: fire in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (0.7.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (0.28.1)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (2.3.3)\n",
      "Requirement already satisfied: prompt-toolkit in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (3.0.52)\n",
      "Requirement already satisfied: pyaml in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (25.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (2.12.5)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (2.32.5)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (1.3.1)\n",
      "Requirement already satisfied: termcolor in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./venv/lib/python3.12/site-packages (from llama-stack-client==0.3.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->llama-stack-client==0.3.0) (3.11)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.3.0) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.3.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama-stack-client==0.3.0) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.3.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.3.0) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.3.0) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas->llama-stack-client==0.3.0) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas->llama-stack-client==0.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas->llama-stack-client==0.3.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas->llama-stack-client==0.3.0) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client==0.3.0) (1.17.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from prompt-toolkit->llama-stack-client==0.3.0) (0.2.14)\n",
      "Requirement already satisfied: PyYAML in ./venv/lib/python3.12/site-packages (from pyaml->llama-stack-client==0.3.0) (6.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->llama-stack-client==0.3.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->llama-stack-client==0.3.0) (2.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-stack-client==0.3.0 rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57d0c0-b6ab-4543-ab17-f41b9359a0d1",
   "metadata": {},
   "source": [
    "Now, import the `LlamaStackClient` class, and set the base URL of the Llamastack server (You can get the URL by running `oc get svc -n competitor-analysis`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9748377-f83d-4711-abda-9718bd35bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "import rich\n",
    "\n",
    "# For access from Notebooks within the cluster\n",
    "LS_URL = \"http://llama-stack-dist-service:8321\"\n",
    "\n",
    "# For access from Notebooks external to the cluster, use the route URL instead (oc get route -n competitor-analysis)\n",
    "#LS_URL = \"https://llama-stack-ext-competitor-analysis.apps.ocp.sx7qw.sandbox2219.opentlc.com/\"\n",
    "\n",
    "# Initialize the client\n",
    "client = LlamaStackClient(base_url=LS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacecac9-05d1-49a1-9f2d-598606c72f48",
   "metadata": {},
   "source": [
    "List the models available in this Llamastack instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9ae7b13-286e-43be-85fc-cff2437e76e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://llama-stack-ext-competitor-analysis.apps.ocp.sx7qw.sandbox2219.opentlc.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Model</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'granite-embedding-125m'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'embedding_dimension'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">api_model_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'embedding'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'sentence-transformers'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'ibm-granite/granite-embedding-125m-english'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">model_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'embedding'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Model</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'vllm-inference/granite-3-3-8b-instruct'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">api_model_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'vllm-inference'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'granite-3-3-8b-instruct'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">model_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Model</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'sentence-transformers/nomic-ai/nomic-embed-text-v1.5'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'embedding_dimension'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'default_configured'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">api_model_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'embedding'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'sentence-transformers'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'nomic-ai/nomic-embed-text-v1.5'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">model_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'embedding'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'granite-embedding-125m'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'embedding_dimension'\u001b[0m: \u001b[1;36m768.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mapi_model_type\u001b[0m=\u001b[32m'embedding'\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'sentence-transformers'\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'model'\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'ibm-granite/granite-embedding-125m-english'\u001b[0m,\n",
       "        \u001b[33mmodel_type\u001b[0m=\u001b[32m'embedding'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'vllm-inference/granite-3-3-8b-instruct'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mapi_model_type\u001b[0m=\u001b[32m'llm'\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'vllm-inference'\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'model'\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'granite-3-3-8b-instruct'\u001b[0m,\n",
       "        \u001b[33mmodel_type\u001b[0m=\u001b[32m'llm'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'sentence-transformers/nomic-ai/nomic-embed-text-v1.5'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'embedding_dimension'\u001b[0m: \u001b[1;36m768.0\u001b[0m, \u001b[32m'default_configured'\u001b[0m: \u001b[3;92mTrue\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mapi_model_type\u001b[0m=\u001b[32m'embedding'\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'sentence-transformers'\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'model'\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'nomic-ai/nomic-embed-text-v1.5'\u001b[0m,\n",
       "        \u001b[33mmodel_type\u001b[0m=\u001b[32m'embedding'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "rich.print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f00c41-1ba7-4e0b-8cac-baa3fb617806",
   "metadata": {},
   "source": [
    "Now, list the providers available in this Llamastack instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4a089a8-b23e-4992-a5ef-fc6b46be4fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://llama-stack-ext-competitor-analysis.apps.ocp.sx7qw.sandbox2219.opentlc.com/v1/providers \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ProviderInfo</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">api</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'inference'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'max_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096.0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'api_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'tls_verify'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">health</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'status'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Not Implemented'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Provider does not implement health check'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'vllm-inference'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'remote::vllm'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ProviderInfo</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">api</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'inference'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">health</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'status'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Not Implemented'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Provider does not implement health check'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'sentence-transformers'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'inline::sentence-transformers'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ProviderInfo</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">api</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'vector_io'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'db_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/opt/app-root/src/.llama/distributions/rh/milvus.db'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'kvstore'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'sqlite'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'namespace'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'db_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/opt/app-root/src/.llama/distributions/rh/milvus_registry.db'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">health</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'status'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Not Implemented'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Provider does not implement health check'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'milvus'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'inline::milvus'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mProviderInfo\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mapi\u001b[0m=\u001b[32m'inference'\u001b[0m,\n",
       "        \u001b[33mconfig\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'url'\u001b[0m: \n",
       "\u001b[32m'https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1'\u001b[0m,\n",
       "            \u001b[32m'max_tokens'\u001b[0m: \u001b[1;36m4096.0\u001b[0m,\n",
       "            \u001b[32m'api_token'\u001b[0m: \u001b[32m'********'\u001b[0m,\n",
       "            \u001b[32m'tls_verify'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mhealth\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'status'\u001b[0m: \u001b[32m'Not Implemented'\u001b[0m, \u001b[32m'message'\u001b[0m: \u001b[32m'Provider does not implement health check'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'vllm-inference'\u001b[0m,\n",
       "        \u001b[33mprovider_type\u001b[0m=\u001b[32m'remote::vllm'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mProviderInfo\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mapi\u001b[0m=\u001b[32m'inference'\u001b[0m,\n",
       "        \u001b[33mconfig\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mhealth\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'status'\u001b[0m: \u001b[32m'Not Implemented'\u001b[0m, \u001b[32m'message'\u001b[0m: \u001b[32m'Provider does not implement health check'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'sentence-transformers'\u001b[0m,\n",
       "        \u001b[33mprovider_type\u001b[0m=\u001b[32m'inline::sentence-transformers'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mProviderInfo\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mapi\u001b[0m=\u001b[32m'vector_io'\u001b[0m,\n",
       "        \u001b[33mconfig\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'db_path'\u001b[0m: \u001b[32m'/opt/app-root/src/.llama/distributions/rh/milvus.db'\u001b[0m,\n",
       "            \u001b[32m'kvstore'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'type'\u001b[0m: \u001b[32m'sqlite'\u001b[0m,\n",
       "                \u001b[32m'namespace'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[32m'db_path'\u001b[0m: \u001b[32m'/opt/app-root/src/.llama/distributions/rh/milvus_registry.db'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mhealth\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'status'\u001b[0m: \u001b[32m'Not Implemented'\u001b[0m, \u001b[32m'message'\u001b[0m: \u001b[32m'Provider does not implement health check'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'milvus'\u001b[0m,\n",
       "        \u001b[33mprovider_type\u001b[0m=\u001b[32m'inline::milvus'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... and 15 more providers\n"
     ]
    }
   ],
   "source": [
    "providers = client.providers.list()\n",
    "rich.print(providers[:3])  # Print only first 3 providers\n",
    "print(f\"\\n... and {len(providers) - 3} more providers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c3d9b-de8b-45a1-91fe-c78323e85f00",
   "metadata": {},
   "source": [
    "## Chat Completion (OpenAI-Compatible API)\n",
    "\n",
    "In LlamaStack 0.3.0, use `client.chat.completions.create()` instead of the deprecated `client.inference.chat_completion()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17244c8b-11a3-477f-ace4-54222899e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: vllm-inference/granite-3-3-8b-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://llama-stack-ext-competitor-analysis.apps.ocp.sx7qw.sandbox2219.opentlc.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OpenAIChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-15b9b63726ca441ebd61a3022dc48bb7'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OpenAIChatCompletionChoice</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OpenAIChatCompletionChoiceMessageOpenAIAssistantMessageParam</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The capital city of Mongolia is Ulaanbaatar.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">annotations</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">reasoning_content</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1767771960</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'granite-3-3-8b-instruct'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OpenAIChatCompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">kv_transfer_params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trace_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'75544eecb8ad8c8c882c727cabfddd72'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'span_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'99e93fbf3330473c'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'timestamp'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2026-01-07T07:46:01.249624Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'attributes'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'model_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vllm-inference/granite-3-3-8b-instruct'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'provider_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vllm-inference'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'value'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'unit'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tokens'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trace_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'75544eecb8ad8c8c882c727cabfddd72'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'span_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'99e93fbf3330473c'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'timestamp'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2026-01-07T07:46:01.249654Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'attributes'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'model_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vllm-inference/granite-3-3-8b-instruct'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'provider_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vllm-inference'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'value'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'unit'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tokens'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trace_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'75544eecb8ad8c8c882c727cabfddd72'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'span_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'99e93fbf3330473c'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'timestamp'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2026-01-07T07:46:01.249661Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'attributes'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'model_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vllm-inference/granite-3-3-8b-instruct'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'provider_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vllm-inference'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'value'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'unit'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tokens'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mOpenAIChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-15b9b63726ca441ebd61a3022dc48bb7'\u001b[0m,\n",
       "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mOpenAIChatCompletionChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "            \u001b[33mmessage\u001b[0m=\u001b[1;35mOpenAIChatCompletionChoiceMessageOpenAIAssistantMessageParam\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'The capital city of Mongolia is Ulaanbaatar.'\u001b[0m,\n",
       "                \u001b[33mname\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mannotations\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mreasoning_content\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mstop_reason\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1767771960\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'granite-3-3-8b-instruct'\u001b[0m,\n",
       "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mOpenAIChatCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m15\u001b[0m,\n",
       "        \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m68\u001b[0m,\n",
       "        \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m83\u001b[0m,\n",
       "        \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mprompt_tokens_details\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mprompt_logprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mkv_transfer_params\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'trace_id'\u001b[0m: \u001b[32m'75544eecb8ad8c8c882c727cabfddd72'\u001b[0m,\n",
       "            \u001b[32m'span_id'\u001b[0m: \u001b[32m'99e93fbf3330473c'\u001b[0m,\n",
       "            \u001b[32m'timestamp'\u001b[0m: \u001b[32m'2026-01-07T07:46:01.249624Z'\u001b[0m,\n",
       "            \u001b[32m'attributes'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'model_id'\u001b[0m: \u001b[32m'vllm-inference/granite-3-3-8b-instruct'\u001b[0m, \u001b[32m'provider_id'\u001b[0m: \u001b[32m'vllm-inference'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'metric'\u001b[0m,\n",
       "            \u001b[32m'metric'\u001b[0m: \u001b[32m'prompt_tokens'\u001b[0m,\n",
       "            \u001b[32m'value'\u001b[0m: \u001b[1;36m68\u001b[0m,\n",
       "            \u001b[32m'unit'\u001b[0m: \u001b[32m'tokens'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'trace_id'\u001b[0m: \u001b[32m'75544eecb8ad8c8c882c727cabfddd72'\u001b[0m,\n",
       "            \u001b[32m'span_id'\u001b[0m: \u001b[32m'99e93fbf3330473c'\u001b[0m,\n",
       "            \u001b[32m'timestamp'\u001b[0m: \u001b[32m'2026-01-07T07:46:01.249654Z'\u001b[0m,\n",
       "            \u001b[32m'attributes'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'model_id'\u001b[0m: \u001b[32m'vllm-inference/granite-3-3-8b-instruct'\u001b[0m, \u001b[32m'provider_id'\u001b[0m: \u001b[32m'vllm-inference'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'metric'\u001b[0m,\n",
       "            \u001b[32m'metric'\u001b[0m: \u001b[32m'completion_tokens'\u001b[0m,\n",
       "            \u001b[32m'value'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'unit'\u001b[0m: \u001b[32m'tokens'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'trace_id'\u001b[0m: \u001b[32m'75544eecb8ad8c8c882c727cabfddd72'\u001b[0m,\n",
       "            \u001b[32m'span_id'\u001b[0m: \u001b[32m'99e93fbf3330473c'\u001b[0m,\n",
       "            \u001b[32m'timestamp'\u001b[0m: \u001b[32m'2026-01-07T07:46:01.249661Z'\u001b[0m,\n",
       "            \u001b[32m'attributes'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'model_id'\u001b[0m: \u001b[32m'vllm-inference/granite-3-3-8b-instruct'\u001b[0m, \u001b[32m'provider_id'\u001b[0m: \u001b[32m'vllm-inference'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'metric'\u001b[0m,\n",
       "            \u001b[32m'metric'\u001b[0m: \u001b[32m'total_tokens'\u001b[0m,\n",
       "            \u001b[32m'value'\u001b[0m: \u001b[1;36m83\u001b[0m,\n",
       "            \u001b[32m'unit'\u001b[0m: \u001b[32m'tokens'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Answer: The capital city of Mongolia is Ulaanbaatar.\n"
     ]
    }
   ],
   "source": [
    "# Get the LLM model for inference\n",
    "inference_model_id = next(m.identifier for m in models if m.model_type == \"llm\")\n",
    "print(f\"Using model: {inference_model_id}\")\n",
    "\n",
    "prompt = \"What is the capital of Mongolia?\"\n",
    "\n",
    "# LlamaStack 0.3.0: Use OpenAI-compatible chat.completions.create() API\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    model=inference_model_id  # Note: 'model' not 'model_id'\n",
    ")\n",
    "rich.print(response)\n",
    "\n",
    "# Extract the answer\n",
    "if response.choices and len(response.choices) > 0:\n",
    "    answer = response.choices[0].message.content\n",
    "    print(f\"\\n✅ Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230ed0f",
   "metadata": {},
   "source": [
    "## List Vector Stores (0.3.0 API)\n",
    "\n",
    "In LlamaStack 0.3.0, `vector_dbs` has been renamed to `vector_stores`. The attributes have also changed:\n",
    "- `identifier` → `id`\n",
    "- `vector_db_name` → `name`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae78a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://llama-stack-ext-competitor-analysis.apps.ocp.sx7qw.sandbox2219.opentlc.com/v1/vector_stores \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 1 vector store(s)\n",
      "\n",
      "Vector Store: competitor-docs\n",
      "  ID: vs_06031455-0d5c-420d-9dce-743229452311\n",
      "  Files: 11 total, 11 completed, 0 failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all vector stores using the 0.3.0 API\n",
    "vector_stores = list(client.vector_stores.list())\n",
    "\n",
    "if not vector_stores:\n",
    "    print(\"❌ No vector stores found!\")\n",
    "    print(\"Run the KFP pipeline to ingest documents first\")\n",
    "else:\n",
    "    print(f\"✅ Found {len(vector_stores)} vector store(s)\\n\")\n",
    "    \n",
    "    for vs in vector_stores:\n",
    "        # 0.3.0 API: 'id' is the primary identifier, 'name' is the logical name\n",
    "        vs_id = getattr(vs, 'id', None)\n",
    "        vs_name = getattr(vs, 'name', None)\n",
    "        file_counts = getattr(vs, 'file_counts', None)\n",
    "        \n",
    "        print(f\"Vector Store: {vs_name}\")\n",
    "        print(f\"  ID: {vs_id}\")\n",
    "        if file_counts:\n",
    "            print(f\"  Files: {file_counts.total} total, {file_counts.completed} completed, {file_counts.failed} failed\")\n",
    "        print()\n",
    "        \n",
    "        # Store for later use\n",
    "        target_vector_store = vs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497be82",
   "metadata": {},
   "source": [
    "## RAG Query: Search Vector Store\n",
    "\n",
    "Use `client.vector_stores.search()` to perform semantic search on the indexed documents.\n",
    "\n",
    "**Note:** The legacy `rag_tool.query()` API with `vector_db_ids` parameter may not work correctly with the new vector stores. Use the OpenAI-compatible `vector_stores.search()` instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97c69249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Query: What was the standalone Profit After Tax (PAT) for HDFC Bank in Q2 FY26?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://llama-stack-ext-competitor-analysis.apps.ocp.sx7qw.sandbox2219.opentlc.com/v1/vector_stores/vs_06031455-0d5c-420d-9dce-743229452311/search \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 5 results:\n",
      "\n",
      "[1] Score: 0.9169\n",
      "     ₹ 40.1 bn; down 22% YoY\n",
      "- Net profit after tax of ₹ 1.8 bn compared to profit of ₹ 2.0 bn in the prior year\n",
      "- EPS of ₹ 2.52\n",
      "- Solvency Ratio at 210% as of September 30, 2025\n",
      "\n",
      "## Subsidiaries - Q2FY26 update - HDFC Securities Ltd\n",
      "\n",
      "- 94.11% stake held by the Bank as of September 30, 2025\n",
      "- 7.4 millio...\n",
      "\n",
      "[2] Score: 0.9029\n",
      "     - 400 013. CIN:  L65920MH1994PLC080618\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## NEWS RELEASE\n",
      "\n",
      "HDFC Bank Ltd. HDFC Bank House, Senapati Bapat Marg, Lower Parel, Mumbai - 400 013. CIN:  L65920MH1994PLC080618\n",
      "\n",
      "herein below are in accordance with the accounting standards used in their standalone reporting under the applica...\n",
      "\n",
      "[3] Score: 0.9002\n",
      "    13% YoY and AUM at ₹ 3.6 tn up by 11% YoY\n",
      "- New Business Premium of ₹ 89 bn with new business margin at 24%\n",
      "- Value of new business for the quarter ₹ 10.1 bn\n",
      "- PAT of ₹ 4.5 bn up by 3% YoY\n",
      "- Solvency Ratio at 175% as of September 30, 2025\n",
      "- Embedded value at ₹ 595 bn improved 14% YoY\n",
      "\n",
      "<!-- image -->...\n",
      "\n",
      "[4] Score: 0.8999\n",
      "     exchange &amp; derivatives revenue of ₹ 15.9 billion (₹ 14.6 billion in the corresponding quarter of the previous year), net trading and mark to market gain of ₹ 23.9 billion (₹ 2.9 billion in the corresponding quarter of the previous year) and miscellaneous income, including recoveries and dividen...\n",
      "\n",
      "[5] Score: 0.8998\n",
      "    <!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## GNPA by Segment\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Stable asset quality across segments\n",
      "\n",
      "## Movement of NPAs\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Credit cost and stock of provisions\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "*excludes contingent provision of ₹ 17 billion &amp; floating prov...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your search query\n",
    "query_text = \"What was the standalone Profit After Tax (PAT) for HDFC Bank in Q2 FY26?\"\n",
    "\n",
    "print(f\"🔍 Query: {query_text}\\n\")\n",
    "\n",
    "# Perform semantic search using the 0.3.0 API\n",
    "try:\n",
    "    search_response = client.vector_stores.search(\n",
    "        vector_store_id=target_vector_store.id,\n",
    "        query=query_text,\n",
    "        max_num_results=5\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    if search_response.data and len(search_response.data) > 0:\n",
    "        print(f\"✅ Found {len(search_response.data)} results:\\n\")\n",
    "        \n",
    "        for i, item in enumerate(search_response.data):\n",
    "            # Extract content\n",
    "            content = \"\"\n",
    "            if hasattr(item, 'content'):\n",
    "                if isinstance(item.content, list):\n",
    "                    content = \" \".join([\n",
    "                        c.text if hasattr(c, 'text') else str(c) \n",
    "                        for c in item.content\n",
    "                    ])\n",
    "                else:\n",
    "                    content = str(item.content)\n",
    "            \n",
    "            score = getattr(item, 'score', 'N/A')\n",
    "            \n",
    "            print(f\"[{i+1}] Score: {score:.4f}\")\n",
    "            print(f\"    {content[:300]}...\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"❌ No results found for your query\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Search failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb2235",
   "metadata": {},
   "source": [
    "## RAG-Augmented Generation\n",
    "\n",
    "Now let's use the retrieved context to generate an answer using the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3910275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Generating answer using RAG...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://llama-stack-ext-competitor-analysis.apps.ocp.sx7qw.sandbox2219.opentlc.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer:\n",
      "\n",
      "The standalone Profit After Tax (PAT) for HDFC Bank in Q2 FY26 was ₹ 196.1 bn.\n"
     ]
    }
   ],
   "source": [
    "# Build context from search results\n",
    "context_parts = []\n",
    "if search_response.data:\n",
    "    for item in search_response.data[:3]:  # Use top 3 results\n",
    "        if hasattr(item, 'content'):\n",
    "            if isinstance(item.content, list):\n",
    "                text = \" \".join([c.text if hasattr(c, 'text') else str(c) for c in item.content])\n",
    "            else:\n",
    "                text = str(item.content)\n",
    "            context_parts.append(text)\n",
    "\n",
    "context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Create RAG prompt\n",
    "rag_prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"📝 Generating answer using RAG...\\n\")\n",
    "\n",
    "# Generate answer using the LLM\n",
    "rag_response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
    "    model=inference_model_id\n",
    ")\n",
    "\n",
    "# Display the answer\n",
    "if rag_response.choices and len(rag_response.choices) > 0:\n",
    "    answer = rag_response.choices[0].message.content\n",
    "    print(f\"✅ Answer:\\n\\n{answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
