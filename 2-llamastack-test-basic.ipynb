{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c539b31c-c247-4327-ac08-5aee9659e4fa",
   "metadata": {},
   "source": [
    "This notebook is used to connect to the Llamastack server and do some basic testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0c611-86b3-497a-9ea0-c6925059c319",
   "metadata": {},
   "source": [
    "Let's start by querying the Llamastack server and check if it is healthy and ready to accept requests. First we need to install the llama-stack client using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c377b52-6d0a-4361-b7be-8f0be0e48a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-stack-client==0.2.22 rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57d0c0-b6ab-4543-ab17-f41b9359a0d1",
   "metadata": {},
   "source": [
    "Now, import the `LlamaStackClient` class, and set the base URL of the Llamastack server (You can get the URL by running `oc get svc -n competitor-analysis`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9748377-f83d-4711-abda-9718bd35bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "import rich\n",
    "\n",
    "LS_URL = \"http://llama-stack-dist-service:8321\"\n",
    "\n",
    "# For access from Notebooks external to the cluster, use the route URL instead (oc get route -n competitor-analysis)\n",
    "# LS_URL = \"https://llama-stack-ext-competitor-analysis.apps.cluster-x5jfr.x5jfr.sandbox2053.opentlc.com\"\n",
    "\n",
    "# Initialize the client\n",
    "client = LlamaStackClient(base_url=LS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacecac9-05d1-49a1-9f2d-598606c72f48",
   "metadata": {},
   "source": [
    "List the models available in this Llamastack instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae7b13-286e-43be-85fc-cff2437e76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "rich.print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f00c41-1ba7-4e0b-8cac-baa3fb617806",
   "metadata": {},
   "source": [
    "Now, list the providers available in this Llamastack instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a089a8-b23e-4992-a5ef-fc6b46be4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = client.providers.list()\n",
    "rich.print(providers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c3d9b-de8b-45a1-91fe-c78323e85f00",
   "metadata": {},
   "source": [
    "Let's send a simple prompt to the Granite model running on MaaS via the Llamastack API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17244c8b-11a3-477f-ace4-54222899e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model_id = next(m.identifier for m in models if m.model_type == \"llm\") # Get the Granite model for inference\n",
    "\n",
    "prompt = \"What is the capital of Mongolia?\"\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model_id=inference_model_id\n",
    "    )\n",
    "rich.print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
