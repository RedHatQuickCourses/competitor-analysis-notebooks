{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Query Document Embeddings via LlamaStack\n",
    "\n",
    "This notebook demonstrates how to query document embeddings stored in Milvus using the **LlamaStack API**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "1. **Connect to LlamaStack** - Access the RAG infrastructure\n",
    "2. **List Vector Databases** - See available document collections\n",
    "3. **Semantic Search** - Query documents using natural language\n",
    "4. **Full RAG with LLM** - Get AI-generated answers with source attribution\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- ‚úÖ Documents ingested via KFP pipeline\n",
    "- ‚úÖ Embeddings stored in Milvus (vector DB: `competitor-docs`)\n",
    "- ‚úÖ Running in RHOAI workbench with cluster access\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Required Libraries\n",
    "\n",
    "Install the LlamaStack client and visualization libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q llama-stack-client=0.2.22 rich pandas tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Import Libraries\n",
    "\n",
    "Import all necessary Python libraries for querying and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.types import Document\n",
    "import logging\n",
    "import pandas as pd\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "from IPython.display import display, Markdown as IPyMarkdown\n",
    "import json\n",
    "\n",
    "# Suppress verbose HTTP logs\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# Initialize Rich console for pretty output\n",
    "console = Console()\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configure LlamaStack Connection\n",
    "\n",
    "Set up the connection to LlamaStack service running in the cluster.\n",
    "\n",
    "**Note:** We use the **in-cluster DNS name** since this notebook runs inside OpenShift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaStack service URL (in-cluster)\n",
    "LLAMASTACK_URL = \"http://llama-stack-dist-service.competitor-analysis.svc.cluster.local:8321\"\n",
    "\n",
    "# Vector DB name (logical identifier used in pipeline)\n",
    "VECTOR_DB_NAME = \"competitor-docs\"\n",
    "\n",
    "console.print(Panel.fit(\n",
    "    f\"[bold cyan]LlamaStack URL:[/bold cyan] {LLAMASTACK_URL}\\n\"\n",
    "    f\"[bold cyan]Target Vector DB:[/bold cyan] {VECTOR_DB_NAME}\",\n",
    "    title=\"üîß Configuration\",\n",
    "    border_style=\"cyan\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Connect to LlamaStack\n",
    "\n",
    "Initialize the LlamaStack client and verify connectivity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize client\n",
    "    client = LlamaStackClient(base_url=LLAMASTACK_URL)\n",
    "    \n",
    "    # Test connection by listing models\n",
    "    models = client.models.list()\n",
    "    \n",
    "    console.print(\"[bold green]‚úÖ Successfully connected to LlamaStack![/bold green]\")\n",
    "    console.print(f\"[dim]Found {len(models)} model(s)[/dim]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]‚ùå Connection failed:[/bold red] {e}\")\n",
    "    console.print(\"[yellow]Tip: Ensure LlamaStack service is running in the cluster[/yellow]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Discover Available Models\n",
    "\n",
    "List all models available in LlamaStack (LLM for inference + Embedding model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table for models\n",
    "table = Table(title=\"ü§ñ Available Models\", show_header=True, header_style=\"bold magenta\")\n",
    "table.add_column(\"Model Type\", style=\"cyan\", width=15)\n",
    "table.add_column(\"Model ID\", style=\"yellow\", width=40)\n",
    "table.add_column(\"Details\", style=\"green\")\n",
    "\n",
    "inference_model_id = None\n",
    "embedding_model_id = None\n",
    "embedding_dimension = None\n",
    "\n",
    "for model in models:\n",
    "    model_type = model.model_type\n",
    "    model_id = model.identifier\n",
    "    \n",
    "    details = \"\"\n",
    "    \n",
    "    if model_type == \"llm\":\n",
    "        inference_model_id = model_id\n",
    "        details = \"Used for text generation\"\n",
    "        icon = \"üí¨\"\n",
    "    elif model_type == \"embedding\":\n",
    "        embedding_model_id = model_id\n",
    "        embedding_dimension = model.metadata.get(\"embedding_dimension\", \"N/A\")\n",
    "        details = f\"Dimension: {embedding_dimension}\"\n",
    "        icon = \"üî¢\"\n",
    "    else:\n",
    "        icon = \"‚ùì\"\n",
    "    \n",
    "    table.add_row(f\"{icon} {model_type}\", model_id, details)\n",
    "\n",
    "console.print(table)\n",
    "\n",
    "# Store model IDs for later use\n",
    "print(f\"\\nüí¨ Inference Model: {inference_model_id}\")\n",
    "print(f\"üî¢ Embedding Model: {embedding_model_id} (Dim: {embedding_dimension})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ List Vector Databases\n",
    "\n",
    "Discover all vector databases registered in LlamaStack and locate our target: **`competitor-docs`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all vector DBs\n",
    "vector_dbs = client.vector_dbs.list()\n",
    "\n",
    "if not vector_dbs:\n",
    "    console.print(\"[bold red]‚ùå No vector databases found![/bold red]\")\n",
    "    console.print(\"[yellow]Tip: Run the KFP pipeline to ingest documents first[/yellow]\")\n",
    "else:\n",
    "    # Create table for vector DBs\n",
    "    table = Table(title=\"üì¶ Vector Databases\", show_header=True, header_style=\"bold magenta\")\n",
    "    table.add_column(\"Status\", width=6)\n",
    "    table.add_column(\"Vector DB ID\", style=\"cyan\", width=40)\n",
    "    table.add_column(\"Logical Name\", style=\"yellow\", width=20)\n",
    "    table.add_column(\"Provider\", style=\"green\", width=15)\n",
    "    table.add_column(\"Embedding Model\", style=\"blue\")\n",
    "    \n",
    "    target_vector_db_id = None\n",
    "    \n",
    "    for vdb in vector_dbs:\n",
    "        vdb_id = vdb.identifier\n",
    "        logical_name = getattr(vdb, 'vector_db_name', 'N/A')\n",
    "        provider = vdb.provider_id\n",
    "        emb_model = getattr(vdb, 'embedding_model', 'N/A')\n",
    "        \n",
    "        # Check if this is our target\n",
    "        is_target = (logical_name == VECTOR_DB_NAME or vdb_id == VECTOR_DB_NAME)\n",
    "        status = \"‚úÖ\" if is_target else \"  \"\n",
    "        \n",
    "        if is_target:\n",
    "            target_vector_db_id = vdb_id\n",
    "        \n",
    "        table.add_row(status, vdb_id, logical_name, provider, emb_model)\n",
    "    \n",
    "    console.print(table)\n",
    "    \n",
    "    # Verify we found our target\n",
    "    if target_vector_db_id:\n",
    "        console.print(f\"\\n[bold green]‚úÖ Found target vector DB:[/bold green] {VECTOR_DB_NAME}\")\n",
    "        console.print(f\"[dim]   Milvus Collection ID: {target_vector_db_id}[/dim]\")\n",
    "    else:\n",
    "        console.print(f\"\\n[bold red]‚ùå Vector DB '{VECTOR_DB_NAME}' not found![/bold red]\")\n",
    "        console.print(f\"[yellow]Available: {[getattr(vdb, 'vector_db_name', vdb.identifier) for vdb in vector_dbs]}[/yellow]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Verify Vector DB Setup\n",
    "\n",
    "Ensure we have a valid vector DB to query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_vector_db_id:\n",
    "    console.print(Panel.fit(\n",
    "        f\"[bold green]‚úÖ Ready to Query[/bold green]\\n\\n\"\n",
    "        f\"[cyan]Vector DB Name:[/cyan] {VECTOR_DB_NAME}\\n\"\n",
    "        f\"[cyan]Vector DB ID:[/cyan] {target_vector_db_id}\\n\"\n",
    "        f\"[cyan]Embedding Model:[/cyan] {embedding_model_id}\\n\"\n",
    "        f\"[cyan]Embedding Dimension:[/cyan] {embedding_dimension}\",\n",
    "        title=\"üìä Query Configuration\",\n",
    "        border_style=\"green\"\n",
    "    ))\n",
    "else:\n",
    "    console.print(Panel.fit(\n",
    "        f\"[bold red]‚ùå Cannot proceed - Vector DB not found[/bold red]\\n\\n\"\n",
    "        f\"Please ensure the KFP pipeline has run successfully and ingested documents.\",\n",
    "        title=\"‚ö†Ô∏è Setup Required\",\n",
    "        border_style=\"red\"\n",
    "    ))\n",
    "    raise ValueError(f\"Vector DB '{VECTOR_DB_NAME}' not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîç Semantic Search\n",
    "\n",
    "Now let's query the documents using natural language!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Define Your Query\n",
    "\n",
    "Customize this cell to ask any question about your ingested documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç CUSTOMIZE YOUR QUERY HERE\n",
    "query_text = \"What was the standalone Profit After Tax (PAT) for HDFC Bank in Q2 FY26??\"\n",
    "\n",
    "console.print(Panel.fit(\n",
    "    f\"[bold yellow]{query_text}[/bold yellow]\",\n",
    "    title=\"üîç Your Query\",\n",
    "    border_style=\"yellow\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Execute Semantic Search\n",
    "\n",
    "Query the vector database and retrieve relevant document chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"[cyan]üîÑ Generating query embeddings...[/cyan]\")\n",
    "console.print(\"[cyan]üîç Searching document vectors...[/cyan]\")\n",
    "console.print()\n",
    "\n",
    "try:\n",
    "    # Perform semantic search using LlamaStack RAG tool\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=query_text,\n",
    "        vector_db_ids=[target_vector_db_id]\n",
    "    )\n",
    "    \n",
    "    # Extract results - handle structured response\n",
    "    if hasattr(rag_response, 'content') and rag_response.content:\n",
    "        # Extract text from content items (rag_response.content is a list)\n",
    "        if isinstance(rag_response.content, list):\n",
    "            search_results = \"\\n\".join([\n",
    "                item.text if hasattr(item, 'text') else str(item) \n",
    "                for item in rag_response.content\n",
    "            ])\n",
    "        else:\n",
    "            search_results = str(rag_response.content)\n",
    "        \n",
    "        # Display results in a panel\n",
    "        console.print(Panel(\n",
    "            search_results,\n",
    "            title=\"üìã Search Results\",\n",
    "            border_style=\"green\",\n",
    "            expand=False\n",
    "        ))\n",
    "        \n",
    "        console.print(f\"\\n[bold green]‚úÖ Search completed successfully![/bold green]\")\n",
    "        \n",
    "    else:\n",
    "        console.print(\"[yellow]‚ö†Ô∏è  No results found for your query[/yellow]\")\n",
    "        search_results = None\n",
    "        \n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]‚ùå Search failed:[/bold red] {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    search_results = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü§ñ Full RAG with LLM\n",
    "\n",
    "Go beyond just retrieving documents - get **AI-generated answers** with source attribution!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Configure LLM Instructions\n",
    "\n",
    "Define system instructions for the LLM to:\n",
    "1. Answer based ONLY on retrieved document context\n",
    "2. Maintain factual accuracy and avoid hallucination\n",
    "3. Provide confidence scores and source attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM system instructions for RAG\n",
    "agent_instructions = \"\"\"\n",
    "You are an intelligent assistant that answers user queries.\n",
    "\n",
    "    Instructions:\n",
    "    - Use only the knowledge_search tool to extract information. Ignore all other sources of information.\n",
    "    - Do NOT make up or assume any facts beyond what is given.\n",
    "    - If the answer cannot be found in the provided context, clearly respond with:\n",
    "      \"The information you asked for is not available in the provided documents.\"\n",
    "    - If you cannot find any relevant information for response, do not print the confidence score and do not mention the sources.\n",
    "    - Maintain factual accuracy and logical consistency at all times.\n",
    "    - If there are multiple relevant pieces of information, summarize them precisely and cite their context where applicable.\n",
    "    - Be concise, structured, and neutral ‚Äî avoid speculation or creative elaboration.\n",
    "    - When numerical or factual answers are expected, extract them exactly as stated in the context.\n",
    "    - Do not quote any numerical information in US Dollars. All numbers to be in Indian Rupees (INR).\n",
    "    - Use currency representation for the Indian locale. Use lakhs, crores and not millions or billions \n",
    "    - Put correct commas in currency to reflect the indian locale. 1 Million Rupees (or 10 Lakhs) should be shown as 10,00,000.\n",
    "    - If you find a factual answer to a query, indicate a confidence score (0‚Äì100%) along with the name of the source documents. \n",
    "    If you do not find the information, then do not cite the confidence score or the source.\n",
    "    - The source is available in a field called 'filename' in the context. For all source files that you mention in the response,\n",
    "      Always Replace the .md file extension with .pdf\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "console.print(Panel(\n",
    "    agent_instructions.strip(),\n",
    "    title=\"ü§ñ LLM Instructions\",\n",
    "    border_style=\"blue\"\n",
    "))\n",
    "\n",
    "# Verify we have the inference model\n",
    "if not inference_model_id:\n",
    "    console.print(\"[bold red]‚ùå No inference model available for RAG[/bold red]\")\n",
    "else:\n",
    "    console.print(f\"\\n[green]‚úÖ Ready for RAG queries with model:[/green] {inference_model_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Two-Step RAG Query (with Streaming)\n",
    "\n",
    "**Step 1:** Retrieve relevant document chunks via semantic search  \n",
    "**Step 2:** Generate AI answer by feeding context to the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(f\"[cyan]üí¨ Question: {query_text}[/cyan]\")\n",
    "console.print()\n",
    "\n",
    "try:\n",
    "    # Step 1: Perform semantic search to retrieve relevant document chunks\n",
    "    console.print(\"[cyan]üîç Step 1: Retrieving relevant document chunks...[/cyan]\")\n",
    "    \n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=query_text,\n",
    "        vector_db_ids=[target_vector_db_id]\n",
    "    )\n",
    "    \n",
    "    # Extract retrieved context\n",
    "    if hasattr(rag_response, 'content') and rag_response.content:\n",
    "        if isinstance(rag_response.content, list):\n",
    "            retrieved_context = \"\\n\".join([\n",
    "                item.text if hasattr(item, 'text') else str(item) \n",
    "                for item in rag_response.content\n",
    "            ])\n",
    "        else:\n",
    "            retrieved_context = str(rag_response.content)\n",
    "        \n",
    "        console.print(f\"[green]‚úÖ Retrieved context ({len(retrieved_context)} chars)[/green]\")\n",
    "        \n",
    "        # Show preview of retrieved context\n",
    "        console.print(Panel(\n",
    "            retrieved_context[:500] + \"...\" if len(retrieved_context) > 500 else retrieved_context,\n",
    "            title=\"üìÑ Retrieved Context (Preview)\",\n",
    "            border_style=\"blue\",\n",
    "            expand=False\n",
    "        ))\n",
    "        \n",
    "    else:\n",
    "        console.print(\"[yellow]‚ö†Ô∏è  No relevant documents found![/yellow]\")\n",
    "        retrieved_context = None\n",
    "    \n",
    "    # Step 2: Generate answer using LLM with retrieved context\n",
    "    if retrieved_context:\n",
    "        console.print(\"\\n[cyan]ü§ñ Step 2: Generating AI answer with context...[/cyan]\")\n",
    "        console.rule(\"[bold green]LLM Response\", style=\"green\")\n",
    "        print()\n",
    "        \n",
    "        # Build RAG prompt with instructions, context, and query\n",
    "        rag_prompt = f\"\"\"{agent_instructions}\n",
    "\n",
    "**Retrieved Document Context:**\n",
    "{retrieved_context}\n",
    "\n",
    "**User Question:**\n",
    "{query_text}\n",
    "\n",
    "**Your Answer:**\"\"\"\n",
    "        \n",
    "        # Call inference API directly (no agent, just LLM)\n",
    "        response = client.inference.chat_completion(\n",
    "            model_id=inference_model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": rag_prompt}\n",
    "            ],\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Stream and print the response\n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            if hasattr(chunk, 'event') and hasattr(chunk.event, 'delta'):\n",
    "                delta = chunk.event.delta\n",
    "                # Extract text from delta object\n",
    "                if hasattr(delta, 'text'):\n",
    "                    content = delta.text\n",
    "                elif isinstance(delta, str):\n",
    "                    content = delta\n",
    "                else:\n",
    "                    content = str(delta)\n",
    "                \n",
    "                print(content, end='', flush=True)\n",
    "                full_response += content\n",
    "        \n",
    "        print()\n",
    "        console.rule(style=\"green\")\n",
    "        console.print(\"\\n[bold green]‚úÖ Answer generated successfully![/bold green]\")\n",
    "        \n",
    "    else:\n",
    "        console.print(\"[yellow]‚ö†Ô∏è  Cannot generate answer without context[/yellow]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    console.print(f\"\\n[bold red]‚ùå RAG query failed:[/bold red] {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Quick Query Helper\n",
    "\n",
    "Run this cell repeatedly to test different queries quickly!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_query(question: str, use_agent: bool = False):\n",
    "    \"\"\"\n",
    "    Quick query helper function.\n",
    "    \n",
    "    Args:\n",
    "        question: Your question\n",
    "        use_agent: If True, use LLM agent; if False, just return search results\n",
    "    \"\"\"\n",
    "    console.print(Panel.fit(\n",
    "        f\"[bold yellow]{question}[/bold yellow]\",\n",
    "        title=\"üîç Query\",\n",
    "        border_style=\"yellow\"\n",
    "    ))\n",
    "    \n",
    "    try:\n",
    "        if use_agent:\n",
    "            # Two-step RAG: retrieve context, then generate answer\n",
    "            console.print(\"[cyan]üîç Retrieving context...[/cyan]\")\n",
    "            rag_response = client.tool_runtime.rag_tool.query(\n",
    "                content=question,\n",
    "                vector_db_ids=[target_vector_db_id]\n",
    "            )\n",
    "            \n",
    "            # Extract context\n",
    "            if hasattr(rag_response, 'content') and rag_response.content:\n",
    "                if isinstance(rag_response.content, list):\n",
    "                    context = \"\\n\".join([\n",
    "                        item.text if hasattr(item, 'text') else str(item)\n",
    "                        for item in rag_response.content\n",
    "                    ])\n",
    "                else:\n",
    "                    context = str(rag_response.content)\n",
    "                \n",
    "                console.print(\"[cyan]ü§ñ Generating answer...[/cyan]\\n\")\n",
    "                \n",
    "                # Build prompt with context\n",
    "                prompt = f\"\"\"{agent_instructions}\n",
    "\n",
    "**Retrieved Document Context:**\n",
    "{context}\n",
    "\n",
    "**User Question:**\n",
    "{question}\n",
    "\n",
    "**Your Answer:**\"\"\"\n",
    "                \n",
    "                # Call LLM with context\n",
    "                response = client.inference.chat_completion(\n",
    "                    model_id=inference_model_id,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    stream=True\n",
    "                )\n",
    "                \n",
    "                for chunk in response:\n",
    "                    if hasattr(chunk, 'event') and hasattr(chunk.event, 'delta'):\n",
    "                        delta = chunk.event.delta\n",
    "                        if hasattr(delta, 'text'):\n",
    "                            content = delta.text\n",
    "                        elif isinstance(delta, str):\n",
    "                            content = delta\n",
    "                        else:\n",
    "                            content = str(delta)\n",
    "                        print(content, end='', flush=True)\n",
    "                print()\n",
    "            else:\n",
    "                console.print(\"[yellow]No relevant context found[/yellow]\")\n",
    "        else:\n",
    "            # Just do semantic search\n",
    "            console.print(\"[cyan]üîç Performing semantic search...[/cyan]\\n\")\n",
    "            rag_response = client.tool_runtime.rag_tool.query(\n",
    "                content=question,\n",
    "                vector_db_ids=[target_vector_db_id]\n",
    "            )\n",
    "            if hasattr(rag_response, 'content') and rag_response.content:\n",
    "                # Extract text from content items\n",
    "                if isinstance(rag_response.content, list):\n",
    "                    results_text = \"\\n\".join([\n",
    "                        item.text if hasattr(item, 'text') else str(item)\n",
    "                        for item in rag_response.content\n",
    "                    ])\n",
    "                else:\n",
    "                    results_text = str(rag_response.content)\n",
    "                \n",
    "                console.print(Panel(\n",
    "                    results_text,\n",
    "                    title=\"üìã Results\",\n",
    "                    border_style=\"green\"\n",
    "                ))\n",
    "            else:\n",
    "                console.print(\"[yellow]No results found[/yellow]\")\n",
    "        \n",
    "        console.print(\"\\n[bold green]‚úÖ Done![/bold green]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]‚ùå Error:[/bold red] {e}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# quick_query(\"What is Basel III?\", use_agent=False)  # Just search\n",
    "# quick_query(\"What is Basel III?\", use_agent=True)   # Full AI answer\n",
    "\n",
    "console.print(\"[green]‚úÖ Helper function loaded! Use:[/green]\")\n",
    "console.print('[dim]   quick_query(\"Your question here\", use_agent=False)[/dim]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Try Different Queries\n",
    "\n",
    "Test various questions on your document corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Search only (fast)\n",
    "quick_query(\"What are the minimum capital requirements?\", use_agent=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Full AI-generated answer (slower, more comprehensive)\n",
    "quick_query(\"Calculate the total standalone operating expenses for ICICI Bank for H1-2026 by finding the values for Q1-2026 and Q2-2026 and adding them together..\", use_agent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own query here!\n",
    "quick_query(\"YOUR QUESTION HERE\", use_agent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä Summary\n",
    "\n",
    "## ‚úÖ What We've Accomplished\n",
    "\n",
    "1. ‚úÖ Connected to LlamaStack API\n",
    "2. ‚úÖ Discovered available models and vector databases\n",
    "3. ‚úÖ Performed semantic search on document embeddings\n",
    "4. ‚úÖ Generated AI-powered answers with source attribution\n",
    "5. ‚úÖ Created reusable query helpers\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Next Steps\n",
    "\n",
    "- **Ingest More Documents**: Run the KFP pipeline with new PDFs\n",
    "- **Experiment with Queries**: Try different question types\n",
    "- **Fine-tune Agent Instructions**: Customize response style\n",
    "- **Build Applications**: Use this as a foundation for RAG apps\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **Pipeline Documentation**: Review kfp/README.md\n",
    "- **CLI Tool**: scripts/milvus-cli.py for debugging\n",
    "- **Architecture**: KFP pipeline design documentation\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Querying! üöÄ**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä Summary\n",
    "\n",
    "## ‚úÖ What We've Accomplished\n",
    "\n",
    "1. ‚úÖ Connected to LlamaStack API\n",
    "2. ‚úÖ Discovered available models and vector databases\n",
    "3. ‚úÖ Performed semantic search on document embeddings\n",
    "4. ‚úÖ Generated AI-powered answers with source attribution\n",
    "5. ‚úÖ Created reusable query helpers\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Next Steps\n",
    "\n",
    "- **Ingest More Documents**: Run the KFP pipeline with new PDFs\n",
    "- **Experiment with Queries**: Try different question types\n",
    "- **Fine-tune Agent Instructions**: Customize response style\n",
    "- **Build Applications**: Use this as a foundation for RAG apps\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **Pipeline Documentation**: `kfp/README.md`\n",
    "- **CLI Tool**: `scripts/milvus-cli.py`\n",
    "- **Architecture**: Review the KFP pipeline design\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Querying! üöÄ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
